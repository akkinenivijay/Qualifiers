
%
% sample.tex
% $Id: sample.tex,v 1.1 2006/03/18 00:21:36 johnh Exp johnh $
%


% The default of sigplan-proc-varsize is 9pt, indented paragraphs (acm style)
% For Sensys or other 10pt conference, use the 10pt option
%\documentclass{sigplan-proc-varsize}
% options:
%\documentclass[9pt]{sigplan-proc-varsize}
\documentclass[10pt]{sigplan-proc-varsize}
%\documentclass[noindentedparagraphs]{sigplan-proc-varsize}



% % hack to avoid the ugly ACM paragraph definition
% % => can't leave blank line after this
% (remove comment for this hack)
% \renewcommand{\paragraph}[1]{\vskip 6pt\noindent\textbf{#1 }}

\usepackage{graphicx}
\usepackage{url}


\numberofauthors{1}


\author{
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
%% e-mail address with \email.
\alignauthor Vijay Akkineni \\
        \affaddr{Department of Computer Science and Engineering}\\
        \affaddr{Georgia State University}\\
       \email{vakkineni1@student.gsu.edu}
}

\title{Fault localization in Communication Networks}

%\conferenceinfo{Phd Qualifiers'13,} {September 6, 2013, Atlanta, United States.}
%\CopyrightYear{2013}

\begin{document}

\maketitle

\begin{abstract}
This paper is about fault localization techniques in communication networks worked for Phd qualifiers examination. 
Fault Localization is an important aspect of network fault management and is a process of deducing the source of a failure from the set of observed indicatations. 
It has been an important research area in the field of networking both communication networks and wireless sensor networks.
As commuication networks grow in size and complexity it has been imposing new set of requirements on fault localization. 
Despite the amount of research done in this field we can still argue that it is an area of open for research. 
The paper essentially disucsses the work that has been done in this field in the pas few years with emphasis on the both the papers given for the examination.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Communication Networks, Sensor Network Applications}{Miscellaneous}

\keywords{Fault Localization, Communication Networks, Root Cause Analysis, Causal Inference}

\section{Introduction}
  \label{sec:intro}

Fault diagnosis is an important part of networking. Faults are unavoidable in communication systems but their quick detection and isolation and repair is critical 
for the robustness, reliability and health of the system. When the networks get large and cumbersome automatic fault diagnosis and fault management is a crucial aspect.

A basic taxonomy in this field is mentioned below.

Event, defined as an exceptional condition occurring in the operation of hardware or software of a managed network, is a central concept pertaining to fault diagnosis.

Faults (also referred to as problems or root causes) constitute a class of network events that can cause other events but are not themselves caused by other 
events. Faults may be classified as: (1) permanent, (2) intermittent, and (3) transient. A permanent fault exists in a network until a repair action is taken.
Intermittent faults occur on a discontinuous and periodic basis causing a degradation of service for short periods of time.
However, frequently re-occurring intermittent faults significantly jeopardize service performance. 
Transient faults cause a temporary and minor degradation of service.

Error(Failure) is defined as a discrepancy between a computed, observed, or measured value or condition and a true, specified, or theoretically correct value or condition. 
Error is a consequence of a fault. Faults may or may not cause one or more errors. 
Errors may cause deviation of a delivered service from the specified service, which is visible to the outside world. 
Errors do not need to be directly corrected, and in many cases they are not visible externally. 
However, an error in a network device or software may cause a malfunctioning of dependent network devices or software. 
Thus, errors may propagate within the network causing failures of faultless hardware or software.

Symptoms are external manifestations of failures. They are observed as alarmsâ€”notifications of a potential failure. 
These notifications may originate from management agents via management protocol messages (SNMP trap and CMIP EVENT-REPORT), 
from management systems that monitor the network status, e.g., using command ping, system log-files or character streams sent by external equipments.

Figure 1 shows the concepts described above in an end to end perspective.

The process of fault diagnosis usually involves three steps as cited in \cite{KatzelaThesis:1996}:

\begin{itemize}
  \item Fault detection, a process of capturing on-line indications of network disorder provided by malfunctioning devices or fault detection agents in the 
  form of alarms.
  \item Fault localization (also referred to as fault isolation, alarm/event correlation, and root cause analysis) is a set of observed fault indications is analyzed to 
  find an explanation of the alarms.
  \item Testing is a process that, given a number of possible explanations, determines the actual faults.
  \item Fault Correction, by which we mean not only to diagnose, but
also to repair all faulty components within a network (This includes the testing component).
\end{itemize}

\begin{figure}[h!]
  \caption{Distinction between fault, error and symptom}
  \centering
    \includegraphics[width=0.5\textwidth]{Fig1}
\end{figure}

The difficulty in the fault localization process arise from the ambiguity of the observed set of alarms as same alarm can be generated from multiple different faults 
and multiple alarms can correlate back to the same fault. The incompleteness of the alarm stems from the fact that the alarm doesn't have all the information or 
there is a loss of alarm. Inconsistency among the observed alarms results from device perception of the fault is different form device to device.  

A set of alarms generated by a fault may depend on many factors such as dependencies among network devices, current configurations, services in use since 
fault occurrence, presence of other faults, values of other network parameters, etc. Due to this non-determinism the system knowledge may be subject to inaccuracy 
and inconsistency. Fault evidence may also be inaccurate because of spurious alarms, which are generated by transient problems or as a result of overly sensitive 
fault detection mechanisms. When spurious symptoms may occur, the management system may not be sure which observed alarms should be taken into account in the fault 
localization process. Event management systems should identify and eliminate multiple simultaneous related or unrelated root causes. 

In large networks distributed fault localization process should be performed in distributed fashion. Many researches \cite{Katzela:1995} and \cite{Yemini:1996} have 
concluded that distributed fault localization using a set of event management nodes is a better approach than centralized process. The complexity arised from the nature 
of error propogation, they propogate either horizontally to the peers or vertically from bottom layer to upper layers or afecting higher level services. The errors also
propogate from one management domain to a different management domain making the process even difficult. Therefore the distribution of complexity to distributed fault 
localization processes and inferring form the collective process makes the process computationally feasible and efficient. 

An alarm can insinuate different type of faults that occurred in different communication devices where in fault localization process may not comeup with a 
definitive answer. Few approaches that will be discussed in this paper combine fault localization with testing and fault ocrrection process to validate the hypothesis. 
Therefore there should be some kind of optimality measure or confidence measure that should be employed in measuring the hypothesis that the localization process 
came up with and it could include the lowest cost or min failure probability or some heuristic function which optimizes the process of validating the hypothesis.

Numerous works have been proposed on fault localization process. The techniques are dervice from different areas of artificial intelligence, graph theory, 
neural networks, automata theory and many other approaches. Figure 2 broadly classifies the exisiting solutions. The two papers provided for the exam falls into the 
category of end to end testing scenarios and proababilistic reasoning and derive their roots from bayesian network analysis. 

\begin{figure}[h!]
  \caption{Classification of fault localization techniques}
  \centering
    \includegraphics[width=0.5\textwidth]{Fig2}
\end{figure}

\section{Expert Systems techniques}
Most widely used technique in the field of fault localization and diagnosis are expert systems as they try to mimic the actions of human expert. 
Most expert systems use rule based system as their inference engine. \cite{Peng:97},\cite{Liu:99} and \cite{Nygate:95} are examples of expert systems.

The expert systems developed differ in the knowledge they use. Rule based fault localization solely depend on the 
structure of the knowledge base as rule definition language. In \cite{Lor:93}   the knowledge base is divided to reusable 
knowledge modeled as core knowledge and customized knowledge.  in  \cite{Liu:99} the rules are organized as composite events and 
an intervention of human expert is required to update the event base.  The rule based systems can act as a powerful tool to eliminate 
least likely hypothesis. Although the RBR paradigm is appropriate for problem-solving tasks that are confined and well-understood its limitations are.

\begin{itemize}
  \item an in-ability to learn from experience.
  \item Fan inability to deal with novel problems.
  \item the difxulty of updating the systems to keep up with rapidiy changing domains such as expanding heterogeneous network.
\end{itemize}

in Model bases expert systems like \cite{Nygate:95} conditions are ususally accosiated with rules which includes predicates referring to system model. 
These predicates test the syetem for existence of relationship among system component. \cite{Nygate:95} 
uses correlation tree skeletons describing cause and effect relation ships between event. regardless of what expert system is being used localization process is 
always driven by inference engine and correaltion rules between events. 

\cite{Lewis:93} and \cite{Gardner:96} are examples of Case-based reasoning systems. The goals of CBR systems are (i) to learn from experience, (ii) to 
offer solutions to novel problems based on past experience, and (iii) to avoid expensive maintenance. The basic idea of CBR is to recall, adapt, and 
execute episodes of former problem-solving in an attempt to deal with a current problem. Former episodes of problem-solving are represented as cases in a case 
library. When confronted with a new problem, a CBR system retrieves a similar case and tries to adapt the case in an attempt to solve the outstanding problem. 
The disadvantages of such a system is the time complexity in retrieving a case that is the closest match to the event and the close tailoring of the application 
to the domain.

In addition to the above mentioned techniques there are other notable techniques are neural networks based approaches \cite{Gardner:97} \cite{Gardner:98} and 
decision tree based approaches \cite{Rodosek:98}. Neural networks have parallel computing architecture and are very fast avoiding bottlenecks which 
commonly arises from serial processing. But the main disadvantage is that the learning process requires intesive training  and in communication networks 
where all the alarms signatures are not readily available for pattern recognition.

Decision tree aproaches are simple and allows expressive representation of expert knowledge howeever they are limited by the dependencies sepcific 
applications nad degraded accuracy in noisy scenarios \cite{Russell:96} \cite{Koller:10}.

\section{Model traversing techniques}
\cite{Katker:96}, \cite{Katker:971}, \cite{Katker:97} and \cite{Gruschke:98} are all Model Traversing that use formal representation of communication 
system with clearly marked relationships across network entities. This technique identifies faults by traversing a model of entities starting form the entity whic reported an alarm and also involves a fault identification process to identify and locate faulty network entities.

The model representation used by this technique is object-oriented representation of the given system. It is based on the OSI management framework and uses GDMO (Guidelines for  Definition  of Managed Objects) with extensions to the MO to model dependencies and services between the entities. This approach can make automated testing possible which can test for various testing entities availability and quality of service standards. 

The event correlation in model traversing techniques is event driven as when an event occurs it is being mapped to the reported entity and the managed object representing that entity. For every event the search begins from the MO reporting the event and is searched recursively forllowing all relationship links between MO's using fault localization algorithms.  Fault localization algorithms used in this technique can be of various flavours.  models every event as singleton classes and merges them whenever they are traced to the same node. Typical proerties of such techniques involve 

\begin{itemize}
  \item Level at which a particular fault has occurred in the model.
  \item Event Type.
  \item Event severity.
  \item Event Origin - to identify if the event occurred is primary or secondary.
\end{itemize}

The MO's provide functions which lets the process test the entity for its operational status.  The root cause is found when the process stops at an entity and validates that it is a malfucntioning object and doesn't depend on any other object. In a multi layer model vertical search is performed first and then horizontal search in the next lower layer to check its peer at the end of search process votes are collected to identify the faulty elements and the device that recieves the most votes is declared faulty and a testing process kicks in to test the validity of the hypothesis. 

Model traversing techniques are pretty robust agianst network configuration changes and very useful in the scenario's where automated testing is a requirement and the depedency model is very natural as they are modeled as Trees or Graphs. 

The biggest drawback is the Fault Propagation Patterns and when there  fault is a logical combination of multiple devices and byzatine problems. It also incurs heavy testing costs. 

\section{Graph-theoretic techniques}

 The interaction beetween the mobile client and server is carried over the webservices which are designed in a restful manner.  The restful services are deployed on a NodeJs platform that is built on Chrome's Javascript Runtime for easily building fast scalable network applications. NodeJs uses an event-driven, non-blocking I/O model that makes it lightweight, effcient and perfect for data-intensive real time applications that run accross distributed devices. Several protocols as HTTP and TCP/IP are built into the node platform. The REST Webservice provides a base url to invoke for a resource (eg: http://trafficanalyzer.gsu.edu/user/123456/updatelocation). When the rest webservice is invoked query data is passed as HTTP body content and is used by the webservice proxy. HTTP post, get, put, delete methods are analogous to create, read, update and delete of a resource. The data passed from the mobile client to the server would be of JSON format example({"facebookid":1432214523423,"firstname":"vijay",
"lastname":"akkineni","latitiude": 34.304567,"longitude":-84.056784,"routeinfo":[binarydata]}).
 
The data passed in via restful urls are parsed and saved in the NoSql Database HBase.  HBase is an open source non relational, distributed database. HBase is not a replacement for the traditional database but it is a column oriented database management system that runs on top of Hadoop file system. HBase applications are written in mapReduce style. Hbase system has a set of tables and tables must have aprimary key which will be used to access the data and supports querying only on the primary key.  Traffic Analyzer app uses the Facebook social login identity as the unique primary key for every user and also stores the route information in the column families. Installing Hbase and running it on cloud environment has been a challenge to me. To achieve this an Amazom machine instance was created using Hbase installed on the image and the image is provisioned on two nodes. One of the nodes acts as primary node and the other secondary node as HBase needs a master node for house keeping. Since EC2 instances are paid services, as soon as we stop and start the server IP address of the server changes. This issue was resolved by running a DNS server to provide a name service to the instances so that we can use names to resolve rather than IP address. 

\subsection{Client Architecture}

The critical component of \textit{Traffic Analyzer} is the core location component to identify the users geo location. Apple IOS provides CoreLocation API for communicating with device hardware to get the users location. Core location supports magnetometer, direction in which the device is pointing and also whether the device is moving also known as its course. The API has several classes to handle the heading and course information in its direction-related API calls.

The key aspect of Traffic Analyzer is to detect the hardware support and act accordingly on applications behaviour. However, location framework does not supports at the API level the availability of hardware, instead the hardware requirements must be specified at the application level before deploying onto the device. The hardware of interest for the requirements are AGPS, Compass, WiFi, Cellular Reception.

The Mapping needs of the application are met by the Mapkit and Google maps frameworks of IOS and Android respectively. These frameworks allows us to embed maps on the application, provide support to add annotations and overlays on top of it, show users location on the map and tracking modes.

\subsection{Application Functionality}
The location framework is an abstraction on top of three main methods of geolocation. The least accuracy level is obtained from the cellular network triangulation method to locate the user and provides a position upto 12km and error can be reduced to 2-3 km based on the closest tower. The next level is provided by the Wifi. This is very precise for about 100m, however the user has to be connected to a wireless hotspot. Finally the highest level of accuracy is provided by GPS and has the error less that 40m. The cost and energy consumption also varies linearly with the decrease of error component in the above hardware. The two parameters that are used int he application are \textit{distanceFilter} and \textit{desiredAccuracy}. These parameters lets the application know on how frequently it will recieve location updates and how accurate the updated readings are.

\begin{table}[ht]
\caption{Desired Accuracy Values for Traffic Analyzer}
\centering
\begin{tabular}{l p{2cm}} \hline
Constant Value & Definition \\
%heading
\hline
kCLLocationAccuracyBestForNavigation & Standard accuracy intended for navigational apps. \\
kCLLocationAccuracyBest & Use highest accuracy available. \\
kCLLocationAccuracyNearestTenMeters & 10 meters accuracy. \\
kCLLocationAccuracyHundredMeters & 100 meters accuracy. \\
kCLLocationAccuracyKilometer & Accuracy upto 1km. \\
kCLLocationAccuracyThreeKilometers & Accuracy upto 3km. \\
\hline
\end{tabular}
\label{table:desaccuracy}
\end{table}

In Traffic Analyzer saving energy is a primary deisgn goal and consumption is reduced based on the hardware device we choose for getting the location info. Our application checks the battery level and when the energy levels of the battery depletes to 30\% of the total energy it fallsback to the least accurate mode of wifi or cell provider.

The application starts by user entering the target address where he is headed and a route planning is performed. Once the input address is forward geocoded so that address is translated to latitude and longitude. Once the driving directions and user route plan is finalized by selecting  one of the routes from the driving directions. The figure 2,3 and 4 describes this behaviour. Planning will now be performed to decide when to perform polling and where. Polling here refers to the querying of the device GPS to fetch the location information. Once querying the device hardware for the location info, this information is now transferred to the server using the restful services that are exposed.

\begin{figure}[h!]
  \caption{Address Input}
  \centering
    \includegraphics[scale=0.4]{address}
\end{figure}

\begin{figure}[h!]
  \caption{Calculate Route}
  \centering
    \includegraphics[scale=0.4]{CalcRoute}
\end{figure}

\begin{figure}[h!]
  \caption{Driving Directions for Polling Planning}
  \centering
    \includegraphics[scale=0.4]{RoutePrep}
\end{figure}

The polling logic currently is very basic and would need further refinement based on statistics. Polling is achieved by using dispatch async that would lets us call a thread and dispatch after can be used in conjuction to achieve this functionality. The current logic is based upon speed and distance filter. NSTimer is an useful function provided by IOS which enables us to configure timer based on an interval. This timer is used to trigger the dispatch queue threads that perform atomic operations of updating the location information in the server.

\begin{itemize}
\item If the destination is above 5 miles and the current speed of the vehicle is above 50 MPH polling is performed only every 15 secs and the distance filter is increased to 500m such that we do not have to perform querying often.
\item If the user is travellign at speeds less than 10MPH polling interval is set to 15secs and distance filter is set to 20m.
\item If the user speed is in between 15-50, then polling interval is set to 5, 8 and 12 secs respectively for 20mph,30mph,40mph.
\end{itemize}

The coverage area of the mobile is limitied so in this scenario the GPS would be able to read the position but with the lack of coverage data provider the server calls would fail. To overcome this kind of failure a SQL Lite database on the mobile is used to store temporary information that needs to be passed to the server.  The structure of the table in SQL Lite database is very similar to the information that is passed in to the restful services. The table is called TEMP\_USER\_LOC\_INFO which has the attribtues of latitude, longitude and other route related info of speed, direction and etc.

The traffic rendering part on the maps is achieved via overlay's on the map. On the load of the screen to view traffic a webservice pulls all the coordinates on the route intersection and draws overlay on top of the view. The traffic is rendered as red, yellow and green paths along the coordinates.

\begin{figure}[h!]
  \caption{Overlay Rendering of Traffic}
  \centering
    \includegraphics[scale=0.4]{traffic}
\end{figure}

\subsection{Kalman Filtering}
Kalman Filter is also known as linear quadratic estimation is an algorithm used to measure a series of measurements observed over the time and containing noise (random variations) and other inaccuracies, and produces estimates of unknown variables that tend to be more precise than those based on a single measurement alone.

The state transition equation for velocity would be 
\begin{math}
V_n = V_{n-1} + w_n
\end{math}. W represents the noise in the system. Next we prepare kalman filter inputs and constants and the the algorithm proceeds in the two steps in the first step it predicts the next data for velocity and in the second step thee estimates are updated with the weighted average of the actual measurement involved. Kalman filter is implemented on the server side however there are still issues with implementation of kalman filter on how to run it on demand and update the data variables.

\section{Timeline of Project Proposal}
Timeline:
\begin{itemize}
\item Research on infrastructure to generate application on multiple smart phones. ( 2 weeks)
\item Enable Facebook Social Network based login for identity purpose. (1 week)
\item Setting up cloud infrastructure and using a Map Reduced based Hbase NoSQL to store User data. (2 weeks)
\item Integrating communication from the Application to the Cloud. (1 week)
\item Rendering overlay of vehicles and traffic on the Map. ( 2 weeks)
\item Experimenting with Kalman Filtering to predict state. (1 week)
\end{itemize}

\section{Conclusions and Future Work}
 The Traffic Analyzer project was able to successfully track user movement and store them in a databse and render the traffic pattern on the maps. The project was able to meet the proposed requirements. There are still a lot of areas to improve in the project. The current polling based mechanism is very primitive and polling can still be improved with optimization methodologies and statistics driven approaches and the kalman filtering is currently is not producing accurate results. Kalman filtering can smooth data in the abscence of data. Further evaluation is needed in mapping solutions  whcih provide ease of drwaing overlays and integration with various other data sources apart from the sensors should be researched.  Project repo can be found at https://github.com/akkinenivijay/TrafficAnalyzer

%\end{document} 

\section{Evaluation}
My evaluation criterion for the project depends on the goals I desired to achieve. The goal for me is to achieve A in this course, upon the delivery of the Mobile App communicating with cloud infrastructure and rendering of the data on Google Maps. This is the brunt of the project and requires significant effort. Enabling social integration and kalman filtering are additional characteristics. Overall I am happy with the learning outcome from this project as I learned both Android, IOS Systems and Map API's.

\appendix
%\bibliographystyle{abbrv}
%\bibliography{sigproc}

\input{sensys10-sample-10pt.bbl}

\end{document}
